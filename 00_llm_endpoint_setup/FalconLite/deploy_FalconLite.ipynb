{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt TGI container to long context\n",
    "Reference: https://huggingface.co/amazon/FalconLite\n",
    "\n",
    "This notebook requires a SageMaker Notebook instance as uses docker to customize the TGI container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROLE = role\n",
    "HF_MODEL_ID = \"amazon/FalconLite\"\n",
    "REPO_NAME = \"falcon-lctx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package FalconLight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install huggingface_hub hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "model_tar_dir = Path(HF_MODEL_ID.split(\"/\")[-1])\n",
    "model_tar_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download model from Hugging Face into model_dir\n",
    "if os.path.exists(model_tar_dir):\n",
    "    snapshot_download(\n",
    "        HF_MODEL_ID,\n",
    "        local_dir=str(model_tar_dir),\n",
    "        revision=\"main\",\n",
    "        local_dir_use_symlinks=False,\n",
    "        ignore_patterns=[\n",
    "            \"*.msgpack*\",\n",
    "            \"*.h5*\",\n",
    "            \"*.bin*\",\n",
    "        ],  # to load safetensor weights only\n",
    "    )\n",
    "\n",
    "# check if safetensor weights are downloaded and available\n",
    "assert len(list(model_tar_dir.glob(\"*.safetensors\"))) > 0, \"Model download failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "parent_dir=os.getcwd()\n",
    "# change to model dir\n",
    "os.chdir(str(model_tar_dir))\n",
    "# use pigz for faster and parallel compression\n",
    "!tar -cf model.tar.gz --use-compress-program=pigz *\n",
    "# change back to parent dir\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "s3_model_uri = S3Uploader.upload(\n",
    "    local_path=str(model_tar_dir.joinpath(\"model.tar.gz\")),\n",
    "    desired_s3_uri=f\"s3://{sess.default_bucket()}/{model_tar_dir}\",\n",
    ")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd build-container; bash build.sh {REPO_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! docker image rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "def get_aws_region():\n",
    "    # Get the current AWS region from the default session\n",
    "    session = boto3.session.Session()\n",
    "    return session.region_name\n",
    "\n",
    "\n",
    "def get_aws_account_id():\n",
    "    # Get the current AWS account ID from the default session\n",
    "    sts_client = boto3.client(\"sts\")\n",
    "    response = sts_client.get_caller_identity()\n",
    "    return response[\"Account\"]\n",
    "\n",
    "\n",
    "REGION = get_aws_region()\n",
    "ACCOUNT_ID = get_aws_account_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_image = f\"{ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com/{REPO_NAME}\"\n",
    "custom_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {ROLE}\")\n",
    "print(f\"MODEL_S3_LOCATION: {s3_model_uri}\")\n",
    "\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "num_gpu = 4\n",
    "\n",
    "health_check_timeout = 600\n",
    "\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
    "    \"SM_NUM_GPUS\": json.dumps(num_gpu),\n",
    "    \"MAX_INPUT_LENGTH\": json.dumps(12000),\n",
    "    \"MAX_TOTAL_TOKENS\": json.dumps(12001),\n",
    "    \"HF_MODEL_QUANTIZE\": \"gptq\",\n",
    "    \"TRUST_REMOTE_CODE\": json.dumps(True),\n",
    "    \"MAX_BATCH_PREFILL_TOKENS\": json.dumps(12001),\n",
    "    \"MAX_BATCH_TOTAL_TOKENS\": json.dumps(12001),\n",
    "    \"GPTQ_BITS\": json.dumps(4),\n",
    "    \"GPTQ_GROUPSIZE\": json.dumps(128),\n",
    "    \"DNTK_ALPHA_SCALER\": json.dumps(0.25),\n",
    "}\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(f\"falconlite-g5-{num_gpu}gpu\")\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=ROLE, image_uri=custom_image, env=config, model_data=s3_model_uri\n",
    ")\n",
    "\n",
    "llm_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    wait=False,\n",
    ")\n",
    "\n",
    "print(f\"Endpointname: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "def call_endpoint(text: str, endpoint_name: str):\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "    parameters = {\n",
    "        \"max_new_tokens\": 250,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": None,\n",
    "        \"typical_p\": 0.2,\n",
    "        \"use_cache\": True,\n",
    "        \"seed\": 1,\n",
    "    }\n",
    "\n",
    "    payload = {\"inputs\": text, \"parameters\": parameters}\n",
    "\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "\n",
    "    output = json.loads(response[\"Body\"].read().decode())\n",
    "\n",
    "    result = output[0][\"generated_text\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "prompt_template = \"<|prompter|>{text}<|endoftext|><|assistant|>\"\n",
    "text = \"What are the main challenges to support a long context for LLM?\"\n",
    "prompt = prompt_template.format(text=text)\n",
    "print(prompt)\n",
    "\n",
    "result = call_endpoint(prompt, endpoint_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
